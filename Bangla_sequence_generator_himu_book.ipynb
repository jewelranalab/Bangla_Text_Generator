{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# বাংলা  sequence generator\n",
    "\n",
    "এই প্রজেক্টে, বাংলায় ক্যারেক্টার-বাই-ক্যারেক্টার sequence generator তৈরি করা হয়েছে। \n",
    "প্রজেক্টে, ডেটাসেট হিসেবে  প্রয়াত লেখক হুমায়ুন আহমেদ এর বিখ্যাত বই \"হিমু\" এর corpus ইউজ করা হয়েছে। \"হিমু\" বইয়ে প্রায় 115440 ক্যারেক্টার আছে যা ট্রেইন করার জন্য যথেষ্ট।  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   বাংলা টেক্সট ডেটাসেট এর প্রি-প্রসেস\n",
    "\n",
    "প্রথমে, আমরা কিছু প্রি-প্রসেস মেথড ইউজ করবো যা আমাদেরকে বিশাল টেক্সট corpus ট্রেইন করতে সুবিধা দিবে।  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমাদের অরিজিনাল টেক্সট এ 115440 বর্ণ আছে\n"
     ]
    }
   ],
   "source": [
    "# হিমু বইয়ের টেক্সট গুলো read করবো। \n",
    "bangla_text = open('datasets/himu.txt').read()\n",
    "print('আমাদের অরিজিনাল টেক্সট এ ' + str(len(bangla_text)) + ' বর্ণ আছে')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমরা যেহেতু স্বয়ংক্রিয়ভাবে বর্ণ সমন্বয়ে বাংলা শব্দ তৈরি করবো সেহেতু আমাদেরকে শুধুমাত্র বাংলা বর্ন ছাড়া বাকি ক্যারেক্টারগুলো রিমুভ করে দিবো। \n",
    "\n",
    "আমরা কি করতে যাচ্ছি তার একটু ধারণা পাওয়ার জন্য নিচে প্রথম ২০০০ টা বর্ণ প্রিন্ট করছি। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nপ্রসঙ্গ হিমু\\n\\n\\n\\n\\tহিমু আমার প্রিয় চরিত্রের একটি। যখন হিমুকে নিয়ে কিছু লিখি-- নিজেকে হিমু মনে হয়, একধরনের ঘোর অনুভব করি। এই ব্যাপারটা অন্য কোন লেখার সময় তেমন ঘটে না। হিমুকে নিয়ে আমার প্রথম লেখা ময়ুরাক্ষি। ময়ুরাক্ষি লেখার সময় ব্যাপারটা প্রথম লক্ষ করি। দ্বিতীয়বারে লিখলাম রজার ওপাশে। তখনো একই ব্যাপার। কেন এরকম হয়? মানুষ হিসেবে আমি যুক্তিবাদী। হিমু যুক্তিহীন, রহস্যময় জগৎ একজন যুক্তিবাদীকে কেন আকর্ষণ করবে? আমার জানা নেই। যদি কখনও জানতে পারি-- পাঠকদের জানাব।\\n\\n\\n\\n\\tহুমায়ূন আহমেদ\\n\\n\\tএলিফেন্ট রোড\\n\\n\\n\\n\\n\\nকি নাম বললেন আপনার, হিমু?’\\n\\n\\t‘জ্বি, হিমু।’\\n\\n\\t‘হিম থেকে হিমু?’\\n\\n\\t‘জ্বি-না, হিমালয় থেকে হিমু। আমার ভাল নাম হিমালয়।\\n\\n\\t‘ঠাট্টা করছেন?’\\n\\n\\t‘না, ঠাট্টা করছি না।’\\n\\n\\tআমি পাঞ্জাবির পকেট থেকে ম্যাট্রিক সার্টিফিকেট বের করে এগিয়ে দিলাম।\\n\\n\\tহাসিমুখে বললাম, সার্টিফিকেটে লেখা আছে। দেখুন।\\n\\n\\tএষা হতভম্ব হয়ে বলল, আপনি কি সার্টিফিকেট পকেটে নিয়ে ঘুরে বেড়ান?\\n\\n\\t‘জ্বি, সার্টিফিকেটটা পকেটেই রাখি।হিমালয় নাম বললে অনেকেই বিশ্বাস করে না, তখন সার্টিফিকেট দেখাই। ওরা তখন বড় ধরণের ঝাঁকি খায়।’\\n\\n\\tআমি উঠে দাড়ালাম।এষা বলল, আপনি কি চলে যাচ্ছেন?\\n\\n\\t‘হুঁ।’\\n\\n\\t‘এখন যাবেন না। একটু বসুন।’\\n\\n\\tআমার যেহেতু কখনোই কোনো তাড়া থাকে না—আমি বসলাম। রাত ন’টার মতো বাজে। এমন কিছু রাত হয়নি—কিন্তু এ বাড়িতে মনে হচ্ছে নিশুতি। কারো কোনো সাড়াশব্দ নেই। বুড়ো মনে হয় এই ফ্ল্যাটের নয়। পাশের ফ্ল্যাটের।\\n\\n\\tএষা আমার সামনে বসে আছে। তার চোখে অবিশ্বাস এবং কৌতুহল একসঙ্গে খেলা করছে। সে অনেক কিছুই জিজ্ঞেস করতে চাচ্ছে, আবার জিজ্ঞেস করতে ভরসা পাচ্ছে না। আমি তাদের কাছে নিতান্তই অপরিচিত একজন। তার দাদীমা রিকসা থেকে পড়ে মাথা ফাটিয়েছেন। আমি ভদ্রমহিলাকে হাসপাতালে নিয়ে মাথা ব্যান্ডেজ করে বাসায়ে পৌঁছে বেতের সোফায় বসে আছি।এদের কাছে এই হচ্ছে আমার পরিচয়।\\n\\n\\tআমি খানিকটা উপকার করেছি। উপকারের প্রতিদান দিতে না পেরে পরিবারটা একটু অস্বস্তির মধ্যে পড়েছে।ঘরে বোধহয় চা-পাতা নেই। চা-পাতা থাকলে এতক্ষণে চা চলে আসত। প্রায় আধঘণ্টা হয়েছে। এর মধ্যে চা চলে আসার কথা।\\n\\n\\tআমি বললাম, আপনাদের বাসায় চা-পাতা নেই, তাই না?\\n\\n\\tএষা আবারো হকচকিয়ে গেল।বিস্ময় গোপন করতে পারল না। গলায় অনেকখানি বিস্ময় নিয়ে বলল, না, নেই। আমাদের কাজের মেয়েটা দেশে গেছে। ওই বাজার-টাজার করে। চা-পাতা না থাকায় আজ বিকেলে আমি চা খেতে পারি'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bangla_text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা টেক্সট এর মধ্য থেকে নতুন লাইন এবং ট্যাব এর Tag গুলো খুঁজে বের করে সেগুলোকে স্পেস দিয়ে রিপ্লেস করবো। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tabs(text):\n",
    "    \n",
    "    text = text.replace('\\n',' ') \n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('\\r',' ')\n",
    "    text = text.replace('\\u200d',' ')\n",
    "    \n",
    "    text = re.sub(' +',' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "bangla_text = remove_tabs(bangla_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "পুনরায় আমরা প্রথম ২০০০ লাইন প্রিন্ট করলে দেখতে পারবো যে, টেক্সট এ নতুন লাইন এবং ট্যাব এর Tag গুলো স্পেস দ্বারা রিপ্লেস হয়ে গিয়েছে। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' প্রসঙ্গ হিমু হিমু আমার প্রিয় চরিত্রের একটি। যখন হিমুকে নিয়ে কিছু লিখি-- নিজেকে হিমু মনে হয়, একধরনের ঘোর অনুভব করি। এই ব্যাপারটা অন্য কোন লেখার সময় তেমন ঘটে না। হিমুকে নিয়ে আমার প্রথম লেখা ময়ুরাক্ষি। ময়ুরাক্ষি লেখার সময় ব্যাপারটা প্রথম লক্ষ করি। দ্বিতীয়বারে লিখলাম রজার ওপাশে। তখনো একই ব্যাপার। কেন এরকম হয়? মানুষ হিসেবে আমি যুক্তিবাদী। হিমু যুক্তিহীন, রহস্যময় জগৎ একজন যুক্তিবাদীকে কেন আকর্ষণ করবে? আমার জানা নেই। যদি কখনও জানতে পারি-- পাঠকদের জানাব। হুমায়ূন আহমেদ এলিফেন্ট রোড কি নাম বললেন আপনার, হিমু?’ ‘জ্বি, হিমু।’ ‘হিম থেকে হিমু?’ ‘জ্বি-না, হিমালয় থেকে হিমু। আমার ভাল নাম হিমালয়। ‘ঠাট্টা করছেন?’ ‘না, ঠাট্টা করছি না।’ আমি পাঞ্জাবির পকেট থেকে ম্যাট্রিক সার্টিফিকেট বের করে এগিয়ে দিলাম। হাসিমুখে বললাম, সার্টিফিকেটে লেখা আছে। দেখুন। এষা হতভম্ব হয়ে বলল, আপনি কি সার্টিফিকেট পকেটে নিয়ে ঘুরে বেড়ান? ‘জ্বি, সার্টিফিকেটটা পকেটেই রাখি।হিমালয় নাম বললে অনেকেই বিশ্বাস করে না, তখন সার্টিফিকেট দেখাই। ওরা তখন বড় ধরণের ঝাঁকি খায়।’ আমি উঠে দাড়ালাম।এষা বলল, আপনি কি চলে যাচ্ছেন? ‘হুঁ।’ ‘এখন যাবেন না। একটু বসুন।’ আমার যেহেতু কখনোই কোনো তাড়া থাকে না—আমি বসলাম। রাত ন’টার মতো বাজে। এমন কিছু রাত হয়নি—কিন্তু এ বাড়িতে মনে হচ্ছে নিশুতি। কারো কোনো সাড়াশব্দ নেই। বুড়ো মনে হয় এই ফ্ল্যাটের নয়। পাশের ফ্ল্যাটের। এষা আমার সামনে বসে আছে। তার চোখে অবিশ্বাস এবং কৌতুহল একসঙ্গে খেলা করছে। সে অনেক কিছুই জিজ্ঞেস করতে চাচ্ছে, আবার জিজ্ঞেস করতে ভরসা পাচ্ছে না। আমি তাদের কাছে নিতান্তই অপরিচিত একজন। তার দাদীমা রিকসা থেকে পড়ে মাথা ফাটিয়েছেন। আমি ভদ্রমহিলাকে হাসপাতালে নিয়ে মাথা ব্যান্ডেজ করে বাসায়ে পৌঁছে বেতের সোফায় বসে আছি।এদের কাছে এই হচ্ছে আমার পরিচয়। আমি খানিকটা উপকার করেছি। উপকারের প্রতিদান দিতে না পেরে পরিবারটা একটু অস্বস্তির মধ্যে পড়েছে।ঘরে বোধহয় চা-পাতা নেই। চা-পাতা থাকলে এতক্ষণে চা চলে আসত। প্রায় আধঘণ্টা হয়েছে। এর মধ্যে চা চলে আসার কথা। আমি বললাম, আপনাদের বাসায় চা-পাতা নেই, তাই না? এষা আবারো হকচকিয়ে গেল।বিস্ময় গোপন করতে পারল না। গলায় অনেকখানি বিস্ময় নিয়ে বলল, না, নেই। আমাদের কাজের মেয়েটা দেশে গেছে। ওই বাজার-টাজার করে। চা-পাতা না থাকায় আজ বিকেলে আমি চা খেতে পারিনি। ‘আমি কি চা-পাতা এনে দেব?’ ‘না না, আপনাকে আনতে'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bangla_text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমাদের টেক্সট এ অনেকগুলো ক্যারেক্টার বা সিম্বল আছে যেগুলো আমাদের কাছে আসবেনা। তাই সেই সকল\n",
    "স্পেশাল সিম্বল গুলো খুঁজে বের করে সেগুলোকে স্পেস দ্বারা রিপ্লেস করে দিবো। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(txt):\n",
    "    \n",
    "    chars = ['$', '!', '@', '?', '%', '/', '*', '-', '&', '(', ')', '\"', \"'\", ',', ';', ':',\n",
    "             '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0','‘','।','’','—', \n",
    "              '১', '২', '৩', '\\u200c', '–', '“', '”', '…',\n",
    "              'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',\n",
    "             't', 'u', 'w', 'y', 'z', '।', ',', '?','I', 'K', 'R', 'T', 'Y'] \n",
    "    for c in chars:\n",
    "        if c in txt:\n",
    "            txt = txt.replace(c, \" \")\n",
    "\n",
    "    # যদি অতিরিক্ত স্পেস থাকে তাহলে একটা স্পেস এ রিপ্লেস করবে। \n",
    "    txt = txt.replace('  ',' ')\n",
    "    \n",
    "    return txt\n",
    "    \n",
    "bangla_text = remove_symbols(bangla_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "পুনরায় আমরা প্রথম ২০০০ লাইন প্রিন্ট করলে দেখে যাচাই করবো যে আগের থেকে দেখতে সুন্দর দেখাচ্ছে কিনা। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' প্রসঙ্গ হিমু হিমু আমার প্রিয় চরিত্রের একটি যখন হিমুকে নিয়ে কিছু লিখি  নিজেকে হিমু মনে হয় একধরনের ঘোর অনুভব করি এই ব্যাপারটা অন্য কোন লেখার সময় তেমন ঘটে না হিমুকে নিয়ে আমার প্রথম লেখা ময়ুরাক্ষি ময়ুরাক্ষি লেখার সময় ব্যাপারটা প্রথম লক্ষ করি দ্বিতীয়বারে লিখলাম রজার ওপাশে তখনো একই ব্যাপার কেন এরকম হয় মানুষ হিসেবে আমি যুক্তিবাদী হিমু যুক্তিহীন রহস্যময় জগৎ একজন যুক্তিবাদীকে কেন আকর্ষণ করবে আমার জানা নেই যদি কখনও জানতে পারি  পাঠকদের জানাব হুমায়ূন আহমেদ এলিফেন্ট রোড কি নাম বললেন আপনার হিমু  জ্বি হিমু  হিম থেকে হিমু  জ্বি না হিমালয় থেকে হিমু আমার ভাল নাম হিমালয়  ঠাট্টা করছেন  না ঠাট্টা করছি না  আমি পাঞ্জাবির পকেট থেকে ম্যাট্রিক সার্টিফিকেট বের করে এগিয়ে দিলাম হাসিমুখে বললাম সার্টিফিকেটে লেখা আছে দেখুন এষা হতভম্ব হয়ে বলল আপনি কি সার্টিফিকেট পকেটে নিয়ে ঘুরে বেড়ান  জ্বি সার্টিফিকেটটা পকেটেই রাখি হিমালয় নাম বললে অনেকেই বিশ্বাস করে না তখন সার্টিফিকেট দেখাই ওরা তখন বড় ধরণের ঝাঁকি খায়  আমি উঠে দাড়ালাম এষা বলল আপনি কি চলে যাচ্ছেন  হুঁ  এখন যাবেন না একটু বসুন  আমার যেহেতু কখনোই কোনো তাড়া থাকে না আমি বসলাম রাত ন টার মতো বাজে এমন কিছু রাত হয়নি কিন্তু এ বাড়িতে মনে হচ্ছে নিশুতি কারো কোনো সাড়াশব্দ নেই বুড়ো মনে হয় এই ফ্ল্যাটের নয় পাশের ফ্ল্যাটের এষা আমার সামনে বসে আছে তার চোখে অবিশ্বাস এবং কৌতুহল একসঙ্গে খেলা করছে সে অনেক কিছুই জিজ্ঞেস করতে চাচ্ছে আবার জিজ্ঞেস করতে ভরসা পাচ্ছে না আমি তাদের কাছে নিতান্তই অপরিচিত একজন তার দাদীমা রিকসা থেকে পড়ে মাথা ফাটিয়েছেন আমি ভদ্রমহিলাকে হাসপাতালে নিয়ে মাথা ব্যান্ডেজ করে বাসায়ে পৌঁছে বেতের সোফায় বসে আছি এদের কাছে এই হচ্ছে আমার পরিচয় আমি খানিকটা উপকার করেছি উপকারের প্রতিদান দিতে না পেরে পরিবারটা একটু অস্বস্তির মধ্যে পড়েছে ঘরে বোধহয় চা পাতা নেই চা পাতা থাকলে এতক্ষণে চা চলে আসত প্রায় আধঘণ্টা হয়েছে এর মধ্যে চা চলে আসার কথা আমি বললাম আপনাদের বাসায় চা পাতা নেই তাই না এষা আবারো হকচকিয়ে গেল বিস্ময় গোপন করতে পারল না গলায় অনেকখানি বিস্ময় নিয়ে বলল না নেই আমাদের কাজের মেয়েটা দেশে গেছে ওই বাজার টাজার করে চা পাতা না থাকায় আজ বিকেলে আমি চা খেতে পারিনি  আমি কি চা পাতা এনে দেব  না না আপনাকে আনতে হবে না আপনি বসুন আপনি কী করেন  আমি একজন পরিব্রাজক  আপনার কথা বুঝতে পারছি না  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bangla_text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "সো, এখন আমাদের সুন্দর একটি বাংলা ক্যারেক্টার sequence আছে। এখন টোটাল ক্যারেক্টার সংখ্যা এবং টোটাল ইউনিক ক্যারেক্টার সংখ্যা প্রিন্ট করবো। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "এই corpus এ আছে সর্বমোট 107843 টি ক্যারেক্টার \n",
      "এই corpus এ আছে সর্বমোট 61 টি ইউনিক ক্যারেক্টার\n"
     ]
    }
   ],
   "source": [
    "# টোটাল ইউনিক ক্যারেক্টার গণনা \n",
    "chars = sorted(list(set(bangla_text)))\n",
    "\n",
    "print (\"এই corpus এ আছে সর্বমোট \" +  str(len(bangla_text)) + \" টি ক্যারেক্টার \")\n",
    "print (\"এই corpus এ আছে সর্বমোট \" +  str(len(chars)) + \" টি ইউনিক ক্যারেক্টার\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ইনপুট/আউটপুট যুগল তৈরি"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_transform_text(text,window_size,step_size):\n",
    "    # containers for input/output pairs\n",
    "    inputs = [text[i:i+window_size] for i in range(0, len(text)-window_size, step_size)]\n",
    "    outputs = [text[i] for i in range(window_size, len(text), step_size)]    \n",
    "    return inputs,outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "উপরের ফাংশন এর সাহায্যে ইনপুট/আউটপুট যুগল তৈরি করতে পারি। যেমনঃ- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your text window-ing function \n",
    "window_size = 100\n",
    "step_size = 5\n",
    "inputs, outputs = window_transform_text(bangla_text,window_size,step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "নিচে কিছু ইনপুট/আউটপুট যুগল প্রিন্ট করলাম। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = ঙ্গ হিমু হিমু আমার প্রিয় চরিত্রের একটি যখন হিমুকে নিয়ে কিছু লিখি  নিজেকে হিমু মনে হয় একধরনের ঘোর অনু\n",
      "output = ভ\n",
      "--------------\n",
      "input = িতে না পেরে পরিবারটা একটু অস্বস্তির মধ্যে পড়েছে ঘরে বোধহয় চা পাতা নেই চা পাতা থাকলে এতক্ষণে চা চলে আ\n",
      "output = স\n"
     ]
    }
   ],
   "source": [
    "# print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from\n",
    "print('input = ' + inputs[1])\n",
    "print('output = ' + outputs[1])\n",
    "print('--------------')\n",
    "print('input = ' + inputs[302])\n",
    "print('output = ' + outputs[302])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## টেক্সট তৈরি \n",
    "\n",
    "\n",
    "ক্যারেক্টার-বাই-ক্যারেক্টার টেক্সট তৈরি হচ্ছে একটি ক্লাসিফিকেশন প্রবলেম কারণ প্রতিটি আউটপুট হচ্ছে একটি সিঙ্গেল ক্যারেক্টার। তাই প্রেডিকশনের ক্লাসের সংখ্যা হচ্ছে ইউনিক ক্যারেক্টারের সমান।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has 61 unique characters\n",
      "and these characters are \n",
      "[' ', '.', 'ঁ', 'ং', 'ঃ', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'এ', 'ঐ', 'ও', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ', '্', 'ৎ', 'ৗ', 'ড়', 'ঢ়', 'য়']\n"
     ]
    }
   ],
   "source": [
    "# print out the number of unique characters in the dataset\n",
    "chars = sorted(list(set(bangla_text)))\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n",
    "print ('and these characters are ')\n",
    "print (chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5  One-hot encoding characters\n",
    "\n",
    "যেহেতু আমাদের ইউনপুট/আউটপুট যুগল হচ্ছে ক্যারেক্টার সেহেতু বাংলা টেক্সট এর প্রতিটি ক্যারেক্টার সমূহকে তার নিউমেরিক ভ্যালুতে নিতে হবে। এই কাজটি আমরা One-hot ইউকোডিং এর মাধ্যমে করতে পারি। \n",
    "\n",
    "$$অ\\longleftarrow\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right]\\,\\,\\,\\,\\,\\,\\,আ\\longleftarrow\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right]\\,\\,\\,\\,\\,ই\\longleftarrow\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0 \n",
    "\\end{array}\\right]\\cdots$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary is a function mapping each unique character to a unique integer\n",
    "chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "\n",
    "# this dictionary is a function mapping each unique integer back to a unique character\n",
    "indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা আমাদের  ইউনপুট/আউটপুট যুগল তৈরি করে সেগুলোকে One-hot ভেক্টর এ নিয়েছি। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform character-based input/output into equivalent numerical versions\n",
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs, outputs = window_transform_text(text,window_size,step_size)\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and tranform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_indices[char]] = 1\n",
    "        y[i, chars_to_indices[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এই ফাংশনটি raw ক্যারেক্টার নিবে এবং তাদের নিউমেরিক্যাল ভ্যালুতে রুপান্তরিত করবে। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_size = 100\n",
    "step_size = 5\n",
    "X,y = encode_io_pairs(bangla_text,window_size,step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_5'></a>\n",
    "\n",
    "##  RNN মডেল তৈরি\n",
    "\n",
    "আমাদের RNN মডেল এ একটি সিঙ্গেল LSTM হিডেন লেয়ার থাকবে। \n",
    "\n",
    "- layer 1 হচ্ছে ২০০ হিডেন ইউনিট এর একটি LSTM মডিউল  --> নোট input_shape = (window_size,len(chars)) যেখানে, len(chars) = number of unique characters\n",
    "- layer 2 হচ্ছে একটি  linear module, fully connected, with len(chars) hidden units --> যেখানে,  len(chars) = number of unique characters\n",
    "- layer 3 হচ্ছে একটি softmax activation\n",
    "- এই মডিউলে **categorical_crossentropy** loss ব্যবহার করেছি।  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_rrn_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=(window_size, len(chars))))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমাদের মডেল আর্কিটেকচার হচ্ছেঃ- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 200)               209600    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 61)                12261     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 61)                0         \n",
      "=================================================================\n",
      "Total params: 221,861\n",
      "Trainable params: 221,861\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_rrn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ট্রেইনিং\n",
    "\n",
    "প্রথমে আমাদের ডেটাসেট কে ট্রেইনিং এবং টেস্টিং এ বিভক্ত করি। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small subset of our input/output pairs\n",
    "Xtrain = X[:100000,:,:]\n",
    "ytrain = y[:100000,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, Xtrain, ytrain, batch_size=500, epochs=50):\n",
    "    \n",
    "    model.fit(Xtrain, ytrain, batch_size=500, epochs=50,verbose = 1)\n",
    "    \n",
    "    # মডেল সেভ\n",
    "    model.save('result/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21549/21549 [==============================] - 192s - loss: 3.3573   \n",
      "Epoch 2/50\n",
      "21549/21549 [==============================] - 196s - loss: 3.1320   \n",
      "Epoch 3/50\n",
      "21549/21549 [==============================] - 191s - loss: 3.0584   \n",
      "Epoch 4/50\n",
      "21549/21549 [==============================] - 190s - loss: 2.8953   \n",
      "Epoch 5/50\n",
      "21549/21549 [==============================] - 190s - loss: 2.7341   \n",
      "Epoch 6/50\n",
      "21549/21549 [==============================] - 190s - loss: 2.6167   \n",
      "Epoch 7/50\n",
      "21549/21549 [==============================] - 186s - loss: 2.5356   \n",
      "Epoch 8/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.4739   \n",
      "Epoch 9/50\n",
      "21549/21549 [==============================] - 186s - loss: 2.4226   \n",
      "Epoch 10/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.3804   \n",
      "Epoch 11/50\n",
      "21549/21549 [==============================] - 186s - loss: 2.3447   \n",
      "Epoch 12/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.3117   \n",
      "Epoch 13/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.2872   \n",
      "Epoch 14/50\n",
      "21549/21549 [==============================] - 186s - loss: 2.2592   \n",
      "Epoch 15/50\n",
      "21549/21549 [==============================] - 188s - loss: 2.2369   \n",
      "Epoch 16/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.2072   \n",
      "Epoch 17/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.1891   \n",
      "Epoch 18/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.1632   \n",
      "Epoch 19/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.1408   \n",
      "Epoch 20/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.1181   \n",
      "Epoch 21/50\n",
      "21549/21549 [==============================] - 188s - loss: 2.0960   \n",
      "Epoch 22/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.0769   \n",
      "Epoch 23/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.0562   \n",
      "Epoch 24/50\n",
      "21549/21549 [==============================] - 188s - loss: 2.0316   \n",
      "Epoch 25/50\n",
      "21549/21549 [==============================] - 187s - loss: 2.0134   \n",
      "Epoch 26/50\n",
      "21549/21549 [==============================] - 188s - loss: 1.9900   \n",
      "Epoch 27/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.9742   \n",
      "Epoch 28/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.9460   \n",
      "Epoch 29/50\n",
      "21549/21549 [==============================] - 192s - loss: 1.9224   \n",
      "Epoch 30/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.9019   \n",
      "Epoch 31/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.8758   \n",
      "Epoch 32/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.8519   \n",
      "Epoch 33/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.8292   \n",
      "Epoch 34/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.8014   \n",
      "Epoch 35/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.7793   \n",
      "Epoch 36/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.7495   \n",
      "Epoch 37/50\n",
      "21549/21549 [==============================] - 188s - loss: 1.7249   \n",
      "Epoch 38/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.6968   \n",
      "Epoch 39/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.6662   \n",
      "Epoch 40/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.6424   \n",
      "Epoch 41/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.6093   \n",
      "Epoch 42/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.5817   \n",
      "Epoch 43/50\n",
      "21549/21549 [==============================] - 187s - loss: 1.5375   \n",
      "Epoch 44/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.5075   \n",
      "Epoch 45/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.4769   \n",
      "Epoch 46/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.4418   \n",
      "Epoch 47/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.3965   \n",
      "Epoch 48/50\n",
      "21549/21549 [==============================] - 185s - loss: 1.3651   \n",
      "Epoch 49/50\n",
      "21549/21549 [==============================] - 186s - loss: 1.3336   \n",
      "Epoch 50/50\n",
      "21549/21549 [==============================] - 185s - loss: 1.3004   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "training(model, Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we make a given number of predictions (characters) based on this fitted model?   \n",
    "\n",
    "First we predict the next character after following any chunk of characters in the text of length equal to our chosen window size.  Then we remove the first character in our input sequence and tack our prediction onto the end.  This gives us a slightly changed sequence of inputs that still has length equal to the size of our window.  We then feed in this updated input sequence into the model to predict the another character.  Together then we have two predicted characters following our original input sequence.  Repeating this process N times gives us N predicted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that uses trained model to predict a desired number of future characters\n",
    "def predict_next_chars(model,input_chars,num_to_predict):     \n",
    "    # create output\n",
    "    predicted_chars = ''\n",
    "    for i in range(num_to_predict):\n",
    "        # convert this round's predicted characters to numerical input    \n",
    "        x_test = np.zeros((1, window_size, 61))\n",
    "        #print(x_test.shape)\n",
    "        #x_test.reshape(1, 100, 28)\n",
    "        for t, char in enumerate(input_chars):\n",
    "            \n",
    "            x_test[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "        # make this round's prediction\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0]\n",
    "\n",
    "        # translate numerical prediction back to characters\n",
    "        r = np.argmax(test_predict)                           # predict class of each test input\n",
    "        d = indices_to_chars[r] \n",
    "\n",
    "        # update predicted_chars and input\n",
    "        predicted_chars+=d\n",
    "        #print(r)\n",
    "        input_chars+=d\n",
    "        input_chars = input_chars[1:]\n",
    "    return predicted_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_6'></a>\n",
    "\n",
    "We now try a few subsets of the complete text as input - note the length of each must be exactly equal to the window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "্ষি লেখার সময় ব্যাপারটা প্রথম লক্ষ করি দ্বিতীয়বারে লিখলাম রজার ওপাশে তখনো একই ব্যাপার কেন এরকম হয় মা \"\n",
      "\n",
      "predicted chars = \n",
      " বললাম ভাত পারছে আমার সঙ্গে বলল ক্যায়েছেল মারামন্য হবে আমার কেছে আপনি কিন কিনু বললাম আমি বললাম আমি আ\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "িম থেকে হিমু  জ্বি না হিমালয় থেকে হিমু আমার ভাল নাম হিমালয়  ঠাট্টা করছেন  না ঠাট্টা করছি না  আমি পাঞ \"\n",
      "\n",
      "predicted chars = \n",
      "্ষু করা  আমি চল করে  বলল মোঝা হচ্ছে আমার মাজে করছে আমি বললাম আমি আমার জন্যে আপনাকে মায়া দিয়ে বললেন আ\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "য় নাম বললে অনেকেই বিশ্বাস করে না তখন সার্টিফিকেট দেখাই ওরা তখন বড় ধরণের ঝাঁকি খায়  আমি উঠে দাড়ালাম এ \"\n",
      "\n",
      "predicted chars = \n",
      "ষা বললাম চাষা হয়ে এই মাথা খেল ময় দেখে পড়ি  আমি আছে  আপনা কিছে না তা আমার মন্যে আমার জানার মাথা  আপনা\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "খেলা করছে সে অনেক কিছুই জিজ্ঞেস করতে চাচ্ছে আবার জিজ্ঞেস করতে ভরসা পাচ্ছে না আমি তাদের কাছে নিতান্তই \"\n",
      "\n",
      "predicted chars = \n",
      " ভালবেনি ভালার করছে না তাঁর জানে আমি কিনু কিনে বলল মোঝা  আমি ভাললেখন চাচ্ছে না  আমি আমার কিছু জন্যে \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# get an appropriately sized chunk of characters from the text\n",
    "start_inds = [200, 500, 800, 1200]\n",
    "\n",
    "# load in weights\n",
    "#model.load_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n",
    "new_model = load_model('result/test.h5')\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = bangla_text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(new_model,input_chars,num_to_predict = 100)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + ' \"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try the same model on the first 100,000 input/output pairs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A simple way to write output to file\n",
    "f = open('my_test_output.txt', 'w')              # create an output file to write too\n",
    "f.write('this is only a test ' + '\\n')           # print some output text\n",
    "x = 2\n",
    "f.write('the value of x is ' + str(x) + '\\n')    # record a variable value\n",
    "f.close()     \n",
    "\n",
    "# print out the contents of my_test_output.txt\n",
    "f = open('my_test_output.txt', 'r')              # create an output file to write too\n",
    "f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this recording device enables us to safely perform experiments on larger portions of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fit our model to the dataset, then generate text using the trained model in precisely the same generation method applied before on the small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see the sentences below generated to read like Sherlock Holmes by the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "\n",
      "input chars = \n",
      "্ষি লেখার সময় ব্যাপারটা প্রথম লক্ষ করি দ্বিতীয়বারে লিখলাম রজার ওপাশে তখনো একই ব্যাপার কেন এরকম হয় মা\"\n",
      "\n",
      "predicted chars = \n",
      " বললাম ভাত পারছে আমার সঙ্গে বলল ক্যায়েছেল মারামন্য হবে আমার কেছে আপনি কিন কিনু বললাম আমি বললাম আমি আ\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      "নুষ হিসেবে আমি যুক্তিবাদী হিমু যুক্তিহীন রহস্যময় জগৎ একজন যুক্তিবাদীকে কেন আকর্ষণ করবে আমার জানা নেই\"\n",
      "\n",
      "predicted chars = \n",
      " আমি  আমি কে আমার জন্যে না আমার কিছু করে পারছেন  আমা কিছু লাগছে  আপনি বল আম কেন হেন আমার  আমার বাসা \"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      " যদি কখনও জানতে পারি  পাঠকদের জানাব হুমায়ূন আহমেদ এলিফেন্ট রোড কি নাম বললেন আপনার হিমু  জ্বি হিমু  হ\"\n",
      "\n",
      "predicted chars = \n",
      "িমা সাহেব বললাম চা চান না  আমি চা খাব চান চাখে চলে একটা বললাম না হয় সাই দরখার করছে না তাহিয় এক আমি আ\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      "্জাবির পকেট থেকে ম্যাট্রিক সার্টিফিকেট বের করে এগিয়ে দিলাম হাসিমুখে বললাম সার্টিফিকেটে লেখা আছে দেখু\"\n",
      "\n",
      "predicted chars = \n",
      " হিমু ভাই  বলব কথা পার চিনি কিনি আমার মায়ে করে আমার সঙ্গে একটার্টার করে মন্যে আপনাকে দেখে হয়ে আসে আম\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      "ন এষা হতভম্ব হয়ে বলল আপনি কি সার্টিফিকেট পকেটে নিয়ে ঘুরে বেড়ান  জ্বি সার্টিফিকেটটা পকেটেই রাখি হিমাল\"\n",
      "\n",
      "predicted chars = \n",
      "ে হয়  আপনি দেখা হলে হল মোরা দেখা হল  হল  হি  আমার কি আমা কিছু আমি বললাম আমি আমার জন্যে আপনাকে জারবেন\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      " মনে হয় এই ফ্ল্যাটের নয় পাশের ফ্ল্যাটের এষা আমার সামনে বসে আছে তার চোখে অবিশ্বাস এবং কৌতুহল একসঙ্গে \"\n",
      "\n",
      "predicted chars = \n",
      "খেলাম করে একটা কোটা খেলাম ভাড়ি তাক  দেখে পারি  আমি কি আমা কিছু লেখে আপনি বললাম ভালামা বেলাম চাড়া হয়ে\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      " অপরিচিত একজন তার দাদীমা রিকসা থেকে পড়ে মাথা ফাটিয়েছেন আমি ভদ্রমহিলাকে হাসপাতালে নিয়ে মাথা ব্যান্ডেজ\"\n",
      "\n",
      "predicted chars = \n",
      " কেল মারা হয়  আপনার করছে আমি বিসা  আপনি বলে আমার মন্যে আমি কেছে আমার মন্যে সঙ্গে দেখা হল  আমি বল কথা\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      " এষা আবারো হকচকিয়ে গেল বিস্ময় গোপন করতে পারল না গলায় অনেকখানি বিস্ময় নিয়ে বলল না নেই আমাদের কাজের মে\"\n",
      "\n",
      "predicted chars = \n",
      "য়ে আসে  ক্যায়েছেন ই আমার সঙ্গে বস্তার কালেন  আমার কর মন্যে সাহেব সাম না  আমি কিছু বললেন আমি  বস্র কর\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get an appropriately sized chunk of characters from the text\n",
    "start_inds = [200,300,400,600,700,1100,1300,1700]\n",
    "\n",
    "# save output\n",
    "f = open('text_gen_output/RNN_large_textdata_output.txt', 'w')  # create an output file to write too\n",
    "\n",
    "# load weights\n",
    "new_model = load_model('result/test.h5')\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = bangla_text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(new_model,input_chars,num_to_predict = 100)\n",
    "\n",
    "    # print out input characters\n",
    "    line = '-------------------' + '\\n'\n",
    "    print(line)\n",
    "    f.write(line)\n",
    "\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "    f.write(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    predict_line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(predict_line)\n",
    "    f.write(predict_line)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
